# -*- coding: utf-8 -*-
"""blackcoffer-assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fab5ISqldSqGLN2aNIZCxYZiF34krqDz
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# Commented out IPython magic to ensure Python compatibility.
# %cd '/kaggle/input/working-data/20211030 Test Assignment'

pos_score = []
neg_score = []
polarity_score = []
subjectivity_score = []
complex_word_counts = []
Average_Sentence_Length = []
Percentage_of_Complex_Words = []
Fog_Index = []
average_words =[]
cleaned_text_Data = []
personal_pronouns_counts , avg_word_lengths = [] , []
syllable_count_per_words = []
count_cleaned_word = []

import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('cmudict')
from nltk.corpus import cmudict

def Extract_data(path):
    data_input = pd.read_excel(path)
    return data_input

data_input = 'Input.xlsx'
data = Extract_data(data_input)
data.head()

import string

def reading_output(path):
    texts = []
    for file_name in os.listdir(path):
        if file_name.endswith('.txt'):
            file_path = os.path.join(path, file_name)
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read()
                texts.append(text)
    return texts

path1 = 'Output_Extract_text'
extracted_texts = reading_output(path1)
for i in extracted_texts:
    print(i)
    break

"""# 1. Sentimantal Analysis

## Load all stop words
"""

# Function to load stop words from multiple files in a folder
def load_stop_words(folder_path):
    stop_words = set()
    for file_name in os.listdir(folder_path):
        file_path = os.path.join(folder_path, file_name)
        with open(file_path, 'r', encoding='latin-1', errors='ignore') as file:
            stop_words.update(i.lower() for i in file.read().splitlines())
    return stop_words

# Example usage:
stop_words_path = 'StopWords'
stop_words = load_stop_words(stop_words_path)
print(stop_words)

"""## 1.1 Cleaning using Stop Words Lists"""

def clean_text_with_stopwords(text , stop_words):
    tokens = nltk.word_tokenize(text.lower())
    cleaned_words = [word for word in tokens if word not in stop_words and word not in string.punctuation]
    return cleaned_words

for text in extracted_texts:
    cleaned_text = clean_text_with_stopwords(text , stop_words)
    cleaned_text_Data.append(cleaned_text)

print(cleaned_text_Data)

"""## 1.2 Creating dictionary of Positive and Negative words"""

# Function to create positive and negative word dictionaries
def create_positive_negative_dicts(positive_file_path, negative_file_path, stop_words):
    positive_words = set()
    negative_words = set()

    # Read positive words file
    with open(positive_file_path, 'r', encoding='utf-8') as positive_file:
        positive_words = set(positive_file.read().splitlines())

    # Read negative words file
    with open(negative_file_path, 'r', encoding='latin-1') as negative_file:
        negative_words = set(negative_file.read().splitlines())

    # Exclude words found in stop words
    positive_words = positive_words - stop_words
    negative_words = negative_words - stop_words

    return positive_words, negative_words

# Path to positive and negative words files within the MasterDictionary folder
positive_file_path = 'MasterDictionary/positive-words.txt'
negative_file_path = 'MasterDictionary/negative-words.txt'

# Create positive and negative word dictionaries
positive_words, negative_words = create_positive_negative_dicts(positive_file_path, negative_file_path, stop_words)

# Example usage:
print("Positive Words:", positive_words)
print("Negative Words:", negative_words)

"""## 1.3 Extracting Derived variables"""

# Calculate Positive Score
def calculate_positive_score(tokens, positive_words):
    return sum(1 for token in tokens if token in positive_words)

# Calculate Negative Score
def calculate_negative_score(tokens, negative_words):
    return sum(1 for token in tokens if token in negative_words) * -1

# Calculate Polarity Score
def calculate_polarity_score(positive_score, negative_score):
    denominator = positive_score + negative_score + 0.000001
    return (positive_score - negative_score) / denominator

# Calculate Subjectivity Score
def calculate_subjectivity_score(positive_score, negative_score, total_words):
    denominator = total_words + 0.000001
    return (positive_score + negative_score) / denominator

for i in cleaned_text_Data:
    # Calculate scores using the functions defined above
    total_words = len(i)
    a = calculate_positive_score(i, positive_words)
    pos_score.append(a)
    b = calculate_negative_score(i, negative_words)
    neg_score.append(b)
    polarity_score.append(calculate_polarity_score(a, b))
    subjectivity_score.append(calculate_subjectivity_score(a, b, total_words))

# Display the calculated scores
print("Positive Score:", pos_score)

print("Negative Score:", neg_score)
print("Polarity Score:", polarity_score)
print("Subjectivity Score:", subjectivity_score)

"""# 2. Analysis of Readability"""

d = cmudict.dict()
# Function to count syllables in a word
def syllable_count(word):
    if word.lower() in d:
        return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])
    else:
        # Simple syllable count heuristic as fallback
        return max(1, len(word) / 3)

# Function to determine if a word is complex (has more than two syllables)
def is_complex_word(word):
    return syllable_count(word) > 2

def Analysis_of_Readability(tokens , sentences):
    num_sentences = len(sentences)

    # Calculate Average Sentence Length
    avg_sentence_length = len(tokens) / num_sentences if num_sentences > 0 else 0

    # Calculate the number of complex words
    num_complex_words = sum(1 for word in tokens if is_complex_word(word))

    # Calculate Percentage of Complex Words
    percentage_complex_words = num_complex_words / len(tokens) if len(tokens) > 0 else 0

    # Calculate Fog Index
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)

    return avg_sentence_length , percentage_complex_words , fog_index

for text in extracted_texts:
    tokens = nltk.word_tokenize(text)
    # Calculate the number of sentences
    sentences = nltk.sent_tokenize(text)
    a , b , c = Analysis_of_Readability(tokens , sentences)
    Average_Sentence_Length.append(a)
    Percentage_of_Complex_Words.append(b)
    Fog_Index.append(c)

# Display the calculated readability metrics
print("Average Sentence Length:", Average_Sentence_Length)
print("Percentage of Complex Words:", Percentage_of_Complex_Words)
print("Fog Index:", Fog_Index)

"""# 3. Average Number of Words Per Sentence"""

# Function to calculate the average number of words per sentence
def avg_words_per_sentence(text):
    sentences = nltk.sent_tokenize(text)
    total_sentences = len(sentences)
    total_words = 0

    for sentence in sentences:
        words = nltk.word_tokenize(sentence)
        total_words += len(words)

    if total_sentences > 0:
        avg_words_per_sentence = total_words / total_sentences
        return avg_words_per_sentence
    else:
        return 0  # Return 0 if there are no sentences in the text

for text in extracted_texts:
    ans = avg_words_per_sentence(text)
    average_words.append(ans)

print(average_words)

"""# 4. Complex Word Count"""

# Function to count the number of complex words (words with more than two syllables)
def count_complex_words(text):
    words = nltk.word_tokenize(text.lower())
    complex_word_count = sum(1 for word in words if syllable_count(word) > 2)
    return complex_word_count

for text in extracted_texts:
    ans = count_complex_words(text)
    complex_word_counts.append(ans)

print(complex_word_counts)

"""# 5. Word Count"""

# Function to count the total cleaned words in the text
def count_cleaned_words(text , stop_words):
    tokens = nltk.word_tokenize(text.lower())

    # Remove punctuation and stopwords
    cleaned_words = [word for word in tokens if word not in stop_words and word not in string.punctuation]

    # Count the remaining cleaned words
    cleaned_word_count = len(cleaned_words)
    return cleaned_word_count

for text in extracted_texts:
    total_cleaned_words = count_cleaned_words(text , stop_words)
    count_cleaned_word.append(total_cleaned_words)

print(count_cleaned_word)

"""# 6. Syllable Count Per Word"""

import re

# Function to count syllables in a word
def syllable_count_per_word(word):
    # Exceptions for words ending with "es" or "ed"
    exceptions = re.compile(r"(es$|ed$)", re.IGNORECASE)

    # Count vowels in the word, excluding certain exceptions
    vowels = 'aeiouy'
    count = 0
    prev_char = None

    # Check for syllables in the word
    for char in word.lower():
        if char in vowels and (not exceptions.search(word.lower()) or (prev_char and prev_char not in vowels)):
            count += 1
        prev_char = char

    # Return syllable count
    return max(count, 1)  # Return at least 1 syllable if count is 0

for text in extracted_texts:
    syllable_count = syllable_count_per_word(text)
    syllable_count_per_words.append(syllable_count)

print(syllable_count_per_words)

"""# 7. Personal Pronouns"""

# Function to count personal pronouns and calculate average word length
def personal_pronouns(text):
    # Count occurrences of personal pronouns
    pronouns = r"\b(I|we|my|ours|us)\b"
    personal_pronouns_count = len(re.findall(pronouns, text, re.IGNORECASE))

    return personal_pronouns_count

for text in extracted_texts:
    pronouns_count = personal_pronouns(text)
    personal_pronouns_counts.append(pronouns_count)

print(personal_pronouns_counts)

"""# 8.Average Word Length"""

# Tokenize words to calculate average word length
def Avg_word_length(text):
    words = nltk.word_tokenize(text.lower())
    total_characters = sum(len(word) for word in words)
    total_words = len(words)

    # Calculate average word length
    if total_words > 0:
        avg_word_length = total_characters / total_words
    else:
        avg_word_length = 0  # Avoid division by zero

    return avg_word_length

avg_word_lengths = []
for text in extracted_texts:
    avg_length = Avg_word_length(text)
    avg_word_lengths.append(avg_length)

print(avg_word_lengths)

output_df = pd.read_excel('Output Data Structure.xlsx')
output_df.head()

output_df['POSITIVE SCORE'] = pos_score

output_df['NEGATIVE SCORE'] = neg_score
output_df['POLARITY SCORE'] = polarity_score
output_df['SUBJECTIVITY SCORE'] = subjectivity_score
output_df['COMPLEX WORD COUNT'] = complex_word_counts

output_df.columns

output_df['AVG SENTENCE LENGTH'] = Average_Sentence_Length
output_df['PERCENTAGE OF COMPLEX WORDS'] = Percentage_of_Complex_Words
output_df['FOG INDEX'] = Fog_Index
output_df['AVG NUMBER OF WORDS PER SENTENCE'] = average_words
output_df['WORD COUNT'] = count_cleaned_word
output_df['SYLLABLE PER WORD'] = syllable_count_per_words
output_df['PERSONAL PRONOUNS'] = personal_pronouns_counts
output_df['AVG WORD LENGTH'] = avg_word_lengths

output_df.head()

output_df.to_excel('/kaggle/working/Computed_Variables.xlsx', index=False)
print('Text extraction and analysis completed.')